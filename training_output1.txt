x_train shape: (88184, 100, 100, 1)
y_train shape: (88184,)
Number of images in x_train 88184
Number of images in x_test 22046
WARNING:tensorflow:From C:\Users\grace\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 98, 98, 28)        280
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 49, 49, 28)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 67228)             0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               8605312
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_2 (Dense)              (None, 232)               29928
=================================================================
Total params: 8,635,520
Trainable params: 8,635,520
Non-trainable params: 0
_________________________________________________________________
2020-07-03 12:16:06.880420: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Epoch 1/10
88184/88184 [==============================] - 542s 6ms/step - loss: 3.6459 - accuracy: 0.1789
Epoch 2/10
88184/88184 [==============================] - 528s 6ms/step - loss: 3.1170 - accuracy: 0.2522
Epoch 3/10
88184/88184 [==============================] - 531s 6ms/step - loss: 2.8083 - accuracy: 0.3011
Epoch 4/10
88184/88184 [==============================] - 526s 6ms/step - loss: 2.5138 - accuracy: 0.3531
Epoch 5/10
88184/88184 [==============================] - 526s 6ms/step - loss: 2.2244 - accuracy: 0.4095
Epoch 6/10
88184/88184 [==============================] - 526s 6ms/step - loss: 1.9461 - accuracy: 0.4706
Epoch 7/10
88184/88184 [==============================] - 526s 6ms/step - loss: 1.6966 - accuracy: 0.5269
Epoch 8/10
88184/88184 [==============================] - 527s 6ms/step - loss: 1.4869 - accuracy: 0.5773
Epoch 9/10
88184/88184 [==============================] - 525s 6ms/step - loss: 1.3175 - accuracy: 0.6191
Epoch 10/10
88184/88184 [==============================] - 525s 6ms/step - loss: 1.1714 - accuracy: 0.6596
22046/22046 [==============================] - 33s 2ms/step
test loss, test acc: [4.943547619348385, 0.2082463949918747]
Label:  208.0
[[9.65478655e-04 5.06743731e-04 7.05222269e-11 3.36954429e-07
  1.99098187e-03 1.69956897e-04 2.26326215e-11 1.19540107e-03
  7.69230755e-05 1.33411246e-04 9.15573852e-04 6.50985341e-04
  2.37105112e-03 1.30512199e-04 5.21299298e-05 9.74257127e-05
  2.48924312e-06 8.08737695e-06 5.86601527e-06 3.16979276e-05
  5.61909750e-04 1.37463183e-04 3.10480042e-04 1.39152144e-06
  9.01196825e-11 6.56080912e-10 8.23201146e-03 2.03210602e-05
  4.40609595e-03 2.76526691e-07 4.51014365e-11 3.15653463e-03
  3.99321340e-11 5.34859851e-07 7.22064852e-10 4.06103730e-02
  4.43382784e-16 5.94044337e-03 4.36604815e-11 1.18823465e-07
  8.22552367e-13 8.17540666e-08 2.11186294e-11 8.52863211e-03
  1.77613329e-04 5.29926363e-03 6.27517982e-10 2.70250831e-02
  4.32980769e-11 3.13639011e-11 2.78406063e-08 4.35595326e-02
  3.14040210e-14 1.08954101e-03 2.12492992e-11 4.49803956e-05
  2.71492273e-09 5.58120519e-07 5.01358361e-14 3.13959457e-03
  1.34805855e-09 2.85821727e-07 4.12673980e-05 1.81177642e-10
  7.87023051e-12 6.08987699e-04 2.88957105e-11 2.27511628e-04
  2.23750362e-06 4.34400354e-05 5.12136340e-01 2.43880693e-10
  4.60074196e-04 5.08443103e-04 1.42850928e-04 1.45108800e-03
  1.92614133e-03 2.47377939e-05 1.76990464e-10 1.93542751e-11
  4.03795057e-05 2.34233748e-14 5.27617267e-05 4.13119141e-03
  3.83740687e-03 4.85722467e-08 9.67495615e-12 5.88012487e-03
  1.42287288e-03 6.48752823e-02 2.56875273e-06 5.89785714e-07
  2.69039080e-09 1.30777567e-04 3.06315064e-06 1.06649477e-05
  3.77955389e-10 4.57007227e-05 2.19258424e-02 4.86523495e-12
  7.17932126e-04 1.51510775e-13 1.36256961e-11 3.62854014e-04
  7.50827789e-03 1.16791382e-10 3.03630732e-05 8.63734385e-05
  2.20987900e-07 1.88161795e-11 1.18929194e-03 8.14398859e-09
  3.08099762e-03 5.22980932e-03 2.50189878e-05 1.19414646e-03
  2.65897341e-07 3.12901917e-03 3.31240310e-03 1.91406819e-08
  1.69415835e-05 7.94668958e-05 2.17766920e-03 1.46816208e-04
  1.88450053e-04 5.14832034e-04 2.17064447e-03 3.33047501e-09
  2.09778591e-07 9.32916105e-11 2.23704963e-03 1.83451823e-10
  3.45706007e-06 7.26536065e-02 2.51577561e-11 1.49550301e-03
  5.74067002e-04 1.39724330e-08 1.14157800e-07 4.65704253e-10
  6.92270332e-05 7.74064483e-06 1.56679773e-03 3.24472832e-03
  1.18662845e-02 1.64705068e-02 6.59950092e-05 4.29777093e-02
  1.91077237e-10 4.52768710e-03 1.39075025e-12 2.26054453e-05
  3.88324338e-11 1.27791869e-03 7.25949940e-05 1.25388647e-06
  2.22794642e-05 2.00542718e-06 2.40232178e-13 5.97673468e-04
  1.59830973e-03 4.43654670e-07 3.12648225e-03 9.57042801e-09
  5.95239873e-12 4.11087649e-06 4.41943496e-11 8.76362446e-06
  1.18975470e-06 3.79088899e-10 1.62230761e-04 5.78549816e-07
  1.67878345e-04 5.85968919e-05 8.18860644e-05 2.65585142e-03
  8.18964872e-06 8.64227113e-05 1.37049322e-11 8.06243042e-05
  1.06487578e-05 8.91710457e-04 6.15163663e-06 1.46315742e-05
  1.84561155e-04 3.43658879e-09 1.87805664e-08 8.69615380e-10
  1.81142444e-04 5.99634543e-04 1.94730121e-04 1.45117030e-03
  4.25454203e-07 4.99894959e-04 2.02580588e-04 8.12534984e-10
  8.02531544e-13 4.75943205e-04 1.58499597e-06 8.41482266e-13
  4.19138246e-07 4.45676551e-11 1.50122987e-11 1.53359082e-07
  2.50207841e-12 1.20388855e-11 1.36806380e-08 7.13080881e-05
  1.87156140e-03 2.69307437e-10 3.26518759e-08 5.61104425e-05
  2.78705329e-07 1.31913858e-09 6.97004964e-07 2.75526545e-03
  3.49366786e-11 5.99224004e-10 2.35059402e-11 1.24010331e-08
  9.62707493e-03 3.18913771e-05 6.09149620e-09 5.78937983e-12
  1.95103567e-05 4.40599237e-08 4.09922242e-04 1.46941417e-10
  1.73574349e-11 8.40156417e-12 3.46339862e-12 2.45545945e-11]]
70
